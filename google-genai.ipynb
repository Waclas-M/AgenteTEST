{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T17:43:37.846668Z",
     "iopub.status.busy": "2025-04-10T17:43:37.846333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install google-generativeai langchain langchain-google-genai langgraph chromadb\n",
    "\n",
    "!pip install -Uq \"google-genai==1.7.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-10T17:43:52.019244Z",
     "iopub.status.busy": "2025-04-10T17:43:52.018842Z",
     "iopub.status.idle": "2025-04-10T17:43:52.027025Z",
     "shell.execute_reply": "2025-04-10T17:43:52.025681Z",
     "shell.execute_reply.started": "2025-04-10T17:43:52.019208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import Dict, TypedDict\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "genai.__version__\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T17:43:54.400539Z",
     "iopub.status.busy": "2025-04-10T17:43:54.400171Z",
     "iopub.status.idle": "2025-04-10T17:43:54.613368Z",
     "shell.execute_reply": "2025-04-10T17:43:54.612201Z",
     "shell.execute_reply.started": "2025-04-10T17:43:54.400482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hahahahahah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:38:53.373181Z",
     "iopub.status.busy": "2025-04-10T06:38:53.372831Z",
     "iopub.status.idle": "2025-04-10T06:38:53.381911Z",
     "shell.execute_reply": "2025-04-10T06:38:53.380832Z",
     "shell.execute_reply.started": "2025-04-10T06:38:53.373155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Inicjalizacja modelu Gemini przez LangChain\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.3,\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "\n",
    "def explain_python_code(python_code: str) -> str:\n",
    "    \"\"\"\n",
    "    Funkcja przyjmuje kod ≈∫r√≥d≈Çowy w Pythonie i zwraca jego wyja≈õnienie\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Jeste≈õ pomocnym asystentem. Wyja≈õnij krok po kroku, co robi ten kod w Pythonie:\n",
    "        \n",
    "    ```python\n",
    "    {python_code}\n",
    "    Wyja≈õnij dzia≈Çanie ka≈ºdej linijki. \n",
    "    \"\"\" \n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)]) \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T06:39:00.711708Z",
     "iopub.status.busy": "2025-04-10T06:39:00.711300Z",
     "iopub.status.idle": "2025-04-10T06:39:04.863014Z",
     "shell.execute_reply": "2025-04-10T06:39:04.861877Z",
     "shell.execute_reply.started": "2025-04-10T06:39:00.711673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sample_code = \"\"\" prompt = \"Please provide me with 6 newest news From POLSANDODWAS\" baseline_response = client.models.generate_content( model=\"gemini-1.5-flash-001\", contents=[prompt]) print(baseline_response.text) \"\"\" \n",
    "\n",
    "explanation = explain_python_code(sample_code) \n",
    "print(\"\\nüß† Wyja≈õnienie kodu:\\n\") \n",
    "\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasy definiujƒÖce agent√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T17:43:59.200384Z",
     "iopub.status.busy": "2025-04-10T17:43:59.199972Z",
     "iopub.status.idle": "2025-04-10T17:43:59.300822Z",
     "shell.execute_reply": "2025-04-10T17:43:59.299676Z",
     "shell.execute_reply.started": "2025-04-10T17:43:59.200350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "import chromadb\n",
    "\n",
    "# Ustawienie klucza API Gemini\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "for model in genai.list_models():\n",
    "    print(f\"Model: {model.name}\")\n",
    "    print(f\"Supported methods: {model.supported_generation_methods}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T17:44:28.876938Z",
     "iopub.status.busy": "2025-04-10T17:44:28.876603Z",
     "iopub.status.idle": "2025-04-10T17:44:28.882108Z",
     "shell.execute_reply": "2025-04-10T17:44:28.880784Z",
     "shell.execute_reply.started": "2025-04-10T17:44:28.876913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Inicjalizacja trwa≈Çego katalogu dla ChromaDB\n",
    "import os\n",
    "\n",
    "persistent_directory = \"/kaggle/working/chromadb_storage\"\n",
    "os.makedirs(persistent_directory, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T17:47:10.414331Z",
     "iopub.status.busy": "2025-04-10T17:47:10.413892Z",
     "iopub.status.idle": "2025-04-10T17:47:13.562429Z",
     "shell.execute_reply": "2025-04-10T17:47:13.561211Z",
     "shell.execute_reply.started": "2025-04-10T17:47:10.414296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "import chromadb\n",
    "\n",
    "# Ustawienie klucza API Gemini\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Funkcja do generowania embeddingu zdjƒôcia\n",
    "# Poprawiona funkcja do generowania embeddingu zdjƒôcia\n",
    "def generate_image_embedding(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    model = genai.GenerativeModel('models/gemini-1.5-flash')\n",
    "\n",
    "    prompt = \"Generate a descriptive caption for embedding purposes.\"\n",
    "    response = model.generate_content([prompt, image])\n",
    "    \n",
    "    # Wygenerowanie embeddingu na podstawie tekstowego opisu zdjƒôcia\n",
    "    embedding_model = genai.GenerativeModel('models/embedding-001')\n",
    "    embedding = genai.embed_content(\n",
    "        model='models/embedding-001',\n",
    "        content=response.text,\n",
    "        task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "    )\n",
    "    return embedding['embedding']\n",
    "# Funkcja do automatycznego generowania opisu zdjƒôcia przez Gemini\n",
    "def generate_image_caption(image_path):\n",
    "    model = genai.GenerativeModel('models/gemini-1.5-flash')\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    prompt = \"Provide a concise and accurate description of this image.\"\n",
    "    response = model.generate_content([prompt, image])\n",
    "\n",
    "    return response.text\n",
    "\n",
    "# Inicjalizacja bazy ChromaDB\n",
    "chroma_client = chromadb.PersistentClient(path=persistent_directory)\n",
    "collection = chroma_client.get_or_create_collection(name=\"flag2\")\n",
    "\n",
    "# Funkcja do dodawania zdjƒôcia do bazy z automatycznym opisem\n",
    "def add_image_to_db(image_path, image_id):\n",
    "    embedding = generate_image_embedding(image_path)\n",
    "    caption = generate_image_caption(image_path)\n",
    "\n",
    "    collection.add(\n",
    "        embeddings=[embedding],\n",
    "        documents=[caption],\n",
    "        ids=[image_id],\n",
    "        metadatas=[{\"path\": image_path}]\n",
    "    )\n",
    "\n",
    "    print(f\"Image '{image_id}' added to DB with caption: {caption}\")\n",
    "\n",
    "# Przyk≈Çad u≈ºycia:\n",
    "add_image_to_db(\"/kaggle/input/testgcc/IMG_1495.jpeg\", \"flag_pl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T17:49:59.163105Z",
     "iopub.status.busy": "2025-04-10T17:49:59.162709Z",
     "iopub.status.idle": "2025-04-10T17:49:59.666245Z",
     "shell.execute_reply": "2025-04-10T17:49:59.664626Z",
     "shell.execute_reply.started": "2025-04-10T17:49:59.163060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Funkcja do embeddingu zapytania tekstowego\n",
    "def generate_query_embedding(query):\n",
    "    embedding = genai.embed_content(\n",
    "        model=\"models/embedding-001\",\n",
    "        content=query,\n",
    "        task_type=\"RETRIEVAL_QUERY\"\n",
    "    )\n",
    "    return embedding[\"embedding\"]\n",
    "\n",
    "# Agent wyszukujƒÖcy najbli≈ºszy obraz na podstawie prompta\n",
    "def find_image_by_prompt(prompt, collection, top_k=1):\n",
    "    query_embedding = generate_query_embedding(prompt)\n",
    "    \n",
    "    # Wyszukiwanie w bazie\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "\n",
    "    if not results['ids'][0]:\n",
    "        print(\"Brak wynik√≥w.\")\n",
    "        return\n",
    "\n",
    "    image_path = results['metadatas'][0][0]['path']\n",
    "    caption = results['documents'][0][0]\n",
    "    print(f\"Najlepiej pasujƒÖcy opis: {caption}\")\n",
    "\n",
    "    # Wy≈õwietlanie obrazu\n",
    "    image = Image.open(image_path)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(caption)\n",
    "    plt.show()\n",
    "\n",
    "# Przyk≈Çad u≈ºycia:\n",
    "find_image_by_prompt(\"flaga polski\", collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T19:25:53.389786Z",
     "iopub.status.busy": "2025-04-08T19:25:53.389346Z",
     "iopub.status.idle": "2025-04-08T19:25:54.006252Z",
     "shell.execute_reply": "2025-04-08T19:25:54.005141Z",
     "shell.execute_reply.started": "2025-04-08T19:25:53.389751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the CodeExplainer agent\n",
    "explainer_agent = CodeExplainer(name=\"CodeExplainer\", model=\"gemini-1.5-flash\", temperature=0.2)\n",
    "\n",
    "# We will now use the LLM as a \"judge\" to evaluate correctness or validity of the code\n",
    "\n",
    "judge_prompt = f\"\"\"\n",
    "You are a strict yet fair code judge. Evaluate the correctness of the agent's response in the context of the provided code snippet.\n",
    "\n",
    "Consider the following criteria:\n",
    "1. Logic and correctness: Is the solution logically sound and functionally correct?\n",
    "2. Best practices: Does the agent adhere to good coding conventions and best practices?\n",
    "3. Clarity: Is the explanation well-structured, understandable, and adequately justified?\n",
    "\n",
    "Provide:\n",
    "- A single overall score from 1 to 5 (1 = very poor, 5 = excellent).\n",
    "- A concise explanation (1‚Äì2 sentences) summarizing the main reasons for your rating.\n",
    "\n",
    "Code:\n",
    "{sample_code}\n",
    "\n",
    "Agent Response:\n",
    "{explanation}\n",
    "\"\"\"\n",
    "\n",
    " # We directly call the LLM client on our judge prompt \n",
    "judgement_response = explainer_agent.llm.invoke([HumanMessage(content=judge_prompt)]) \n",
    "print(\"\\n=== LLM AS JUDGE ===\\n\") \n",
    "print(judgement_response.content)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7108009,
     "sourceId": 11357591,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
